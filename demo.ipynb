{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echolancer Demo Notebook\n",
    "\n",
    "This notebook demonstrates how to use the Echolancer model for text-to-speech synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath('.')\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Import Echolancer modules\n",
    "from model import Echolancer\n",
    "from utils import get_model, load_checkpoint, get_param_num\n",
    "from utils.metrics import compute_mae\n",
    "from utils.visualization import plot_spectrogram, plot_attention_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration (should match training configuration)\n",
    "config = {\n",
    "    'vocab_size': 100,\n",
    "    'mel_channels': 80,\n",
    "    'emotion_channels': 256,\n",
    "    'speaker_channels': 32,\n",
    "    'multi_speaker': False,\n",
    "    'n_speaker': 1,\n",
    "    'encoder_hidden': 256,\n",
    "    'encoder_head': 4,\n",
    "    'encoder_layer': 4,\n",
    "    'decoder_hidden': 256,\n",
    "    'decoder_layer': 4,\n",
    "    'decoder_head': 4,\n",
    "    'encoder_dropout': 0.1,\n",
    "    'decoder_dropout': 0.1,\n",
    "    'use_alibi': True,\n",
    "    'alibi_alpha': 1.0,\n",
    "    'activation': 'relu'\n",
    "}\n",
    "\n",
    "print(\"Creating Echolancer model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = get_model(**config)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model created successfully\")\n",
    "print(f\"Number of parameters: {get_param_num(model) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_demo(text, vocab_size=100):\n",
    "    \"\"\"Simple tokenizer for demonstration\"\"\"\n",
    "    tokens = []\n",
    "    for char in text.lower():\n",
    "        if char.isalnum():\n",
    "            # Simple hashing for demo purposes\n",
    "            token_id = hash(char) % (vocab_size - 10)\n",
    "            tokens.append(token_id)\n",
    "        elif char == ' ':\n",
    "            tokens.append(vocab_size - 1)\n",
    "    return tokens if tokens else [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_inference(model, text, speaker_id=0, emotion=\"neutral\", max_length=100):\n",
    "    \"\"\"Perform simple inference with the Echolancer model\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Tokenize input text\n",
    "        tokens = tokenize_text_demo(text, config['vocab_size'])\n",
    "        print(f\"Input text: '{text}'\")\n",
    "        print(f\"Token IDs: {tokens[:20]}{'...' if len(tokens) > 20 else ''}\")\n",
    "        \n",
    "        # Prepare tensors\n",
    "        texts = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "        src_lens = torch.tensor([len(tokens)], dtype=torch.long, device=device)\n",
    "        speakers = torch.tensor([speaker_id], dtype=torch.long, device=device)\n",
    "        \n",
    "        # Create emotion embedding (demo purposes)\n",
    "        em_hidden = torch.randn(1, 768, device=device)\n",
    "        \n",
    "        # Run inference\n",
    "        model.eval()\n",
    "        token_outputs = model.infer(\n",
    "            speakers, texts, src_lens, \n",
    "            em_hidden=em_hidden, \n",
    "            max_length=max_length\n",
    "        )\n",
    "        \n",
    "        print(f\"Generated {len(token_outputs[0])} tokens\")\n",
    "        \n",
    "        return token_outputs[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with sample text\n",
    "sample_text = \"Hello, welcome to Echolancer!\"\n",
    "tokens = simple_inference(model, sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_info(model):\n",
    "    \"\"\"Print detailed information about the model\"\"\"\n",
    "    print(\"=== Model Information ===\")\n",
    "    print(f\"Total parameters: {get_param_num(model) / 1e6:.2f}M\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=== Component Breakdown ===\")\n",
    "    print(f\"Encoder parameters: {get_param_num(model.encoder) / 1e6:.2f}M\")\n",
    "    print(f\"Decoder parameters: {get_param_num(model.decoder) / 1e6:.2f}M\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=== Model Architecture ===\")\n",
    "    print(f\"Encoder hidden size: {model.encoder.d_model}\")\n",
    "    print(f\"Encoder heads: {model.encoder.num_heads}\")\n",
    "    print(f\"Encoder layers: {len(model.encoder.encoder.layers)}\")\n",
    "    print(f\"Decoder hidden size: {model.decoder.d_model}\")\n",
    "    print(f\"Decoder heads: {model.decoder.num_heads}\")\n",
    "    print(f\"Decoder layers: {len(model.decoder.dec.layers)}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=== Features ===\")\n",
    "    print(f\"ALiBi support: {hasattr(model.encoder.encoder.layers[0].self_attn, 'use_alibi') and model.encoder.encoder.layers[0].self_attn.use_alibi}\")\n",
    "    print(f\"Emotion conditioning: {model.emotion_channels > 0}\")\n",
    "    print(f\"Multi-speaker support: {model.multi_speaker}\")"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple visualization\n",
    "def visualize_generated_tokens(tokens, max_display=50):\n",
    "    \"\"\"Visualize the generated tokens\"\"\"\n",
    "    \n",
    "    # Limit display for readability\n",
    "    display_tokens = tokens[:max_display]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(15, 4))\n",
    "    \n",
    "    # Plot tokens\n",
    "    ax1.plot(display_tokens, marker='o', linestyle='-', markersize=4)\n",
    "    ax1.set_title('Generated Tokens')\n",
    "    ax1.set_xlabel('Position')\n",
    "    ax1.set_ylabel('Token ID')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Statistics:\")\n",
    "    print(f\"  Mean token ID: {np.mean(display_tokens):.2f}\")\n",
    "    print(f\"  Std token ID: {np.std(display_tokens):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the generated tokens\n",
    "visualize_generated_tokens(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, text=\"This is a benchmark test sentence.\", iterations=5):\n",
    "    \"\"\"Benchmark the model inference speed\"\"\"\n",
    "    import time\n",
    "    \n",
    "    tokens = tokenize_text_demo(text)\n",
    "    texts = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "    src_lens = torch.tensor([len(tokens)], dtype=torch.long, device=device)\n",
    "    speakers = torch.tensor([0], dtype=torch.long, device=device)\n",
    "    em_hidden = torch.randn(1, 768, device=device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(2):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            _ = model.infer(speakers, texts, src_lens, em_hidden=em_hidden, max_length=100)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(iterations):\n",
    "            start_time = time.time()\n",
    "            _ = model.infer(speakers, texts, src_lens, em_hidden=em_hidden, max_length=100)\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    \n",
    "    print(f\"=== Performance Benchmark ===\")\n",
    "    print(f\"Average inference time: {avg_time:.4f}s Â± {std_time:.4f}s\")\n",
    "    print(f\"Inference throughput: {1/avg_time:.2f} samples/sec\")\n",
    "    print(f\"Iterations: {iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark\n",
    "benchmark_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Model Loading**: How to create and load an Echolancer model\n",
    "2. **Simple Inference**: Basic text-to-speech synthesis\n",
    "3. **Model Information**: Displaying model architecture details\n",
    "4. **Visualization**: Plotting generated tokens\n",
    "5. **Performance Testing**: Benchmarking inference speed\n",
    "\n",
    "The Echolancer model provides a full transformer-based text-to-speech pipeline with features like:\n",
    "- Non-autoregressive text encoder\n",
    "- Autoregressive spectrogram decoder\n",
    "- Emotion conditioning\n",
    "- Multi-speaker support\n",
    "- ALiBi attention with linear biases\n",
    "- Mixed precision training support"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}