# Training configuration
optimizer:
  type: "adamw"
  learning_rate: 0.0001
  weight_decay: 0.01
  eps: 1e-06
  betas: [0.9, 0.98]
  grad_clip_thresh: 1.0
  grad_acc_step: 1

scheduler:
  type: "warmup_cosine"
  warmup_steps: 4000

loss:
  pos_weight: 8.0

training:
  batch_size: 16
  epochs: 100
  seed: 1234
  save_step: 1000
  log_step: 100
  val_step: 500
  test_step: 2000
  eval_step: 1000

path:
  ckpt_path: "./ckpt"
  log_path: "./logs"
  result_path: "./results"

# WANDB configuration
wandb:
  project: "echolancer"
  entity: null
  name: "echolancer-run"
  notes: "Training Echolancer model"
  tags: ["text-to-speech", "transformer"]
  group: "experiment"

# Early stopping
early_stopping:
  patience: 10
  min_delta: 0.001

# Testing configuration
testing:
  enabled: true
  bert_model: "answerdotai/ModernBERT-base"
  pre_encoder_dir: "./pre_encoder"
  istftnet_dir: "./istftnet"
  test_sentences: 
    - "This is a test sentence."
    - "Another example sentence."
  test_speakers: [0, 0]
  # Zero-shot VQ tokens for testing (optional)
  # test_vq_tokens: null  # Path to VQ tokens for zero-shot testing
  # test_vq_token_lens: null  # Lengths of VQ token sequences